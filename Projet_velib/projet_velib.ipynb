{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Recommandation des stations velib dans le 1er arrondissement de Paris*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://lenovo-ideapad.home:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1572902045469)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4493afb9\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = 2.4.4\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.DataFrameNaFunctions\n",
       "import org.apache.spark.sql.types._\n",
       "import scala.util.parsing.json._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrameNaFunctions\n",
    "import org.apache.spark.sql.types._\n",
    "import scala.util.parsing.json._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib: org.apache.spark.sql.DataFrame = [Nombre de bornes disponibles: int, Nombre vélo en PARK+1: int ... 19 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val velib: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \";\")\n",
    "      .csv(\"/home/parfait/Documents/MS BGD/Data Mining_SD701/Projet_velib/data/velib-dispo-2810-matin.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 1390\n",
      "Nombre de colonnes : 21\n"
     ]
    }
   ],
   "source": [
    "println(s\"Nombre de lignes : ${velib.count}\")\n",
    "println(s\"Nombre de colonnes : ${velib.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre de bornes disponibles: integer (nullable = true)\n",
      " |-- Nombre vélo en PARK+1: integer (nullable = true)\n",
      " |-- Nombres de bornes en station: integer (nullable = true)\n",
      " |-- PARK + activation: string (nullable = true)\n",
      " |-- densityLevel: integer (nullable = true)\n",
      " |-- Achat possible en station (CB): string (nullable = true)\n",
      " |-- Description station: string (nullable = true)\n",
      " |-- maxBikeOverflow: integer (nullable = true)\n",
      " |-- Etat du Totem: string (nullable = true)\n",
      " |-- nbFreeDock: integer (nullable = true)\n",
      " |-- Nombre de vélo mécanique: integer (nullable = true)\n",
      " |-- PARK +: string (nullable = true)\n",
      " |-- nbDock: integer (nullable = true)\n",
      " |-- Nombre vélo électrique: integer (nullable = true)\n",
      " |-- Nombre vélo en PARK+14: integer (nullable = true)\n",
      " |-- Code de la station: integer (nullable = true)\n",
      " |-- Nom de la station: string (nullable = true)\n",
      " |-- Etat des stations: string (nullable = true)\n",
      " |-- Type de stations: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- duedate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adresses: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val adresses: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \";\")\n",
    "      .csv(\"/home/parfait/Documents/MS BGD/Data Mining_SD701/Projet_velib/data/adresses-75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 151430\n",
      "Nombre de colonnes : 20\n"
     ]
    }
   ],
   "source": [
    "println(s\"Nombre de lignes : ${adresses.count}\")\n",
    "println(s\"Nombre de colonnes : ${adresses.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- id_fantoir: string (nullable = true)\n",
      " |-- numero: integer (nullable = true)\n",
      " |-- rep: string (nullable = true)\n",
      " |-- nom_voie: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- code_insee: integer (nullable = true)\n",
      " |-- nom_commune: string (nullable = true)\n",
      " |-- code_insee_ancienne_commune: string (nullable = true)\n",
      " |-- nom_ancienne_commune: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- alias: string (nullable = true)\n",
      " |-- nom_ld: string (nullable = true)\n",
      " |-- libelle_acheminement: string (nullable = true)\n",
      " |-- nom_afnor: string (nullable = true)\n",
      " |-- source_position: string (nullable = true)\n",
      " |-- source_nom_voie: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adresses.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Cleaning data velib et adresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velibClean: org.apache.spark.sql.DataFrame = [nbre_bornes_dispo: int, nbre_bornes_station: int ... 12 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velibClean: DataFrame =  velib\n",
    "    .withColumn(\"x_velib\", split($\"geo\", \",\").getItem(1).cast(\"double\"))\n",
    "    .withColumn(\"y_velib\", split($\"geo\", \",\").getItem(0).cast(\"double\"))\n",
    "    .withColumnRenamed(\"Nombre de bornes disponibles\",\"nbre_bornes_dispo\")\n",
    "    .withColumnRenamed(\"Nombres de bornes en station\",\"nbre_bornes_station\")\n",
    "    .withColumnRenamed(\"Achat possible en station (CB)\",\"achat_cb\")\n",
    "    .withColumnRenamed(\"Description station\",\"description\")\n",
    "    .withColumnRenamed(\"Etat du Totem\",\"etat_totem\")\n",
    "    .withColumnRenamed(\"Nombre de vélo mécanique\",\"nbre_velos_meca\")\n",
    "    .withColumnRenamed(\"Nombre vélo électrique\",\"nbre_velos_elec\")\n",
    "    .withColumnRenamed(\"Code de la station\",\"cod_station\")\n",
    "    .withColumnRenamed(\"Nom de la station\",\"nom_station\")\n",
    "    .withColumnRenamed(\"Etat des stations\",\"etat_station\")\n",
    "    .withColumnRenamed(\"Type de stations\",\"type_station\")\n",
    "    .drop($\"Nombre vélo en PARK+1\")\n",
    "    .drop($\"PARK + activation\")\n",
    "    .drop($\"Nombre vélo en PARK+14\")\n",
    "    .drop($\"PARK +\")\n",
    "    .drop($\"densityLevel\")\n",
    "    .drop($\"description\")\n",
    "    .drop($\"maxBikeOverflow\")\n",
    "    .drop($\"nbFreeDock\")\n",
    "    .drop($\"nbDock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nbre_bornes_dispo: integer (nullable = true)\n",
      " |-- nbre_bornes_station: integer (nullable = true)\n",
      " |-- achat_cb: string (nullable = true)\n",
      " |-- etat_totem: string (nullable = true)\n",
      " |-- nbre_velos_meca: integer (nullable = true)\n",
      " |-- nbre_velos_elec: integer (nullable = true)\n",
      " |-- cod_station: integer (nullable = true)\n",
      " |-- nom_station: string (nullable = true)\n",
      " |-- etat_station: string (nullable = true)\n",
      " |-- type_station: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- duedate: timestamp (nullable = true)\n",
      " |-- x_velib: double (nullable = true)\n",
      " |-- y_velib: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velibClean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compte tenu des puissances de calcul de mon ordinateur, je travaillerai uniquement sur les adresses du premier arrondissement de Paris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adresses01: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val adresses01: DataFrame = adresses.filter($\"code_postal\" === 75001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adresseClean: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val adresseClean: DataFrame =  adresses01\n",
    "    .drop($\"code_insee_ancienne_commune\")\n",
    "    .drop($\"lon\")\n",
    "    .drop($\"lat\")\n",
    "    .drop($\"alias\")\n",
    "    .drop($\"nom_ld\")\n",
    "    .drop($\"libelle_acheminement\")\n",
    "    .drop($\"nom_afnor\")\n",
    "    .drop($\"source_position\")\n",
    "    .drop($\"source_nom_voie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- id_fantoir: string (nullable = true)\n",
      " |-- numero: integer (nullable = true)\n",
      " |-- rep: string (nullable = true)\n",
      " |-- nom_voie: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- code_insee: integer (nullable = true)\n",
      " |-- nom_commune: string (nullable = true)\n",
      " |-- nom_ancienne_commune: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adresseClean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Recommandation de la station la plus proche de chaque adresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoin: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 23 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoin = adresseClean.crossJoin(velibClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+-------------+-------------+\n",
      "|numero|x        |y         |x_velib      |y_velib      |\n",
      "+------+---------+----------+-------------+-------------+\n",
      "|1     |650852.89|6862571.29|646867.5476  |6863127.31356|\n",
      "|1     |650852.89|6862571.29|651504.905142|6861726.97899|\n",
      "|1     |650852.89|6862571.29|653410.789083|6863281.10064|\n",
      "|1     |650852.89|6862571.29|651401.378236|6864567.90437|\n",
      "|1     |650852.89|6862571.29|655430.484286|6861934.72171|\n",
      "|1     |650852.89|6862571.29|655049.485913|6860263.89269|\n",
      "|1     |650852.89|6862571.29|650472.822069|6861462.23007|\n",
      "|1     |650852.89|6862571.29|651838.493023|6861475.44599|\n",
      "|1     |650852.89|6862571.29|651784.241945|6857907.53976|\n",
      "|1     |650852.89|6862571.29|648747.091267|6864888.51711|\n",
      "|1     |650852.89|6862571.29|647760.557289|6864997.90998|\n",
      "|1     |650852.89|6862571.29|653502.104984|6863633.0448 |\n",
      "|1     |650852.89|6862571.29|650744.673053|6864031.55387|\n",
      "|1     |650852.89|6862571.29|651265.173128|6859924.25257|\n",
      "|1     |650852.89|6862571.29|650894.789258|6861498.38068|\n",
      "|1     |650852.89|6862571.29|652542.486642|6859643.10533|\n",
      "|1     |650852.89|6862571.29|652440.286493|6860622.48571|\n",
      "|1     |650852.89|6862571.29|650198.245367|6860995.8999 |\n",
      "|1     |650852.89|6862571.29|654932.135488|6867998.05607|\n",
      "|1     |650852.89|6862571.29|654238.521674|6857484.44924|\n",
      "+------+---------+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crossJoin.select($\"numero\", $\"x\", $\"y\",$\"x_velib\", $\"y_velib\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoinDistInt: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 25 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoinDistInt: DataFrame = crossJoin\n",
    "    .withColumn(\"dist_km\", round(sqrt(pow($\"x_velib\" - $\"x\", 2) + pow($\"y_velib\" - $\"y\", 2))/1000, 3))\n",
    "    .withColumn(\"proba\",($\"nbre_velos_meca\"/ $\"nbre_bornes_station\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoinDist: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoinDist: DataFrame = crossJoinDistInt\n",
    "    .withColumn(\"esperance\", round(pow($\"dist_km\" + 1, -1) * $\"proba\" ,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inter: org.apache.spark.sql.DataFrame = [id: string, dist_min: double]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inter: DataFrame = crossJoinDist.groupBy(\"id\").agg(min($\"dist_km\").as(\"dist_min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StationProche: org.apache.spark.sql.DataFrame = [id: string, dist_min: double ... 28 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val StationProche: DataFrame = inter.join(crossJoinDist, (crossJoinDist.col(\"id\") ===  inter.col(\"id\") && crossJoinDist.col(\"dist_km\") === inter.col(\"dist_min\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+-------------------------------------+-----------+---------------+-------+\n",
      "|numero|nom_voie                            |nom_station                          |cod_station|nbre_velos_meca|dist_km|\n",
      "+------+------------------------------------+-------------------------------------+-----------+---------------+-------+\n",
      "|163   |Rue Saint-honore                    |Saint-Honoré - Musée du Louvre       |1023       |13             |0.003  |\n",
      "|4     |Rue de Ventadour                    |Ventadour - Opéra                    |1116       |7              |0.004  |\n",
      "|2     |Rue d'Alger                         |Alger - Rivoli                       |1018       |13             |0.004  |\n",
      "|19    |Rue des Halles                      |Halles - Bourdonnais                 |1021       |6              |0.005  |\n",
      "|217   |Rue Saint-honore                    |Saint-Honoré - 29 juillet            |1017       |13             |0.006  |\n",
      "|4     |Rue de la Grande Truanderie         |Grande Truanderie - Saint-Denis      |1006       |7              |0.006  |\n",
      "|25    |Rue des Lavandieres Sainte-opportune|Lavandieres Sainte Opportune - Rivoli|1120       |9              |0.006  |\n",
      "|6     |Rue Francaise                       |Française - Etienne Marcel           |1102       |2              |0.006  |\n",
      "|41    |Qu de l'Horloge                     |Quai de l'Horloge - Pont Neuf        |1001       |6              |0.007  |\n",
      "|188   |Rue Saint-honore                    |Place du Palais Royal                |1013       |32             |0.007  |\n",
      "+------+------------------------------------+-------------------------------------+-----------+---------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "StationProche.select($\"numero\", $\"nom_voie\", $\"nom_station\", $\"cod_station\", $\"nbre_velos_meca\", $\"dist_km\")\n",
    "    .distinct().sort($\"dist_km\".asc).show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Recommandation de la station ayant la plus forte espérance (Espérance basée sur la distance à la station et la probabilité d'avoir un vélo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esperances: org.apache.spark.sql.DataFrame = [id: string, esperance_max: double]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esperances: DataFrame = crossJoinDist.groupBy(\"id\").agg(max($\"esperance\").as(\"esperance_max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationForteEsp: org.apache.spark.sql.DataFrame = [id: string, esperance_max: double ... 28 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationForteEsp: DataFrame = esperances.join(crossJoinDist, (crossJoinDist.col(\"id\") ===  esperances.col(\"id\") && crossJoinDist.col(\"esperance\") === esperances.col(\"esperance_max\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------------------------+-----------+---------------+-------+---------+\n",
      "|numero|nom_voie          |nom_station                 |cod_station|nbre_velos_meca|dist_km|esperance|\n",
      "+------+------------------+----------------------------+-----------+---------------+-------+---------+\n",
      "|37    |Rue Etienne Marcel|Etienne Marcel - Montorgueil|2002       |17             |0.01   |16.832   |\n",
      "|35    |Rue Etienne Marcel|Etienne Marcel - Montorgueil|2002       |17             |0.019  |16.683   |\n",
      "|29    |Rue Montorgueil   |Etienne Marcel - Montorgueil|2002       |17             |0.033  |16.457   |\n",
      "|39    |Rue Etienne Marcel|Etienne Marcel - Montorgueil|2002       |17             |0.033  |16.457   |\n",
      "|27    |Rue Montorgueil   |Etienne Marcel - Montorgueil|2002       |17             |0.038  |16.378   |\n",
      "|25    |Rue Montorgueil   |Etienne Marcel - Montorgueil|2002       |17             |0.043  |16.299   |\n",
      "|40    |Rue Montorgueil   |Etienne Marcel - Montorgueil|2002       |17             |0.047  |16.237   |\n",
      "|23    |Rue Montorgueil   |Etienne Marcel - Montorgueil|2002       |17             |0.049  |16.206   |\n",
      "|38    |Rue Montorgueil   |Etienne Marcel - Montorgueil|2002       |17             |0.051  |16.175   |\n",
      "|33    |Rue Etienne Marcel|Etienne Marcel - Montorgueil|2002       |17             |0.052  |16.16    |\n",
      "+------+------------------+----------------------------+-----------+---------------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stationForteEsp.select($\"numero\", $\"nom_voie\", $\"nom_station\", $\"cod_station\", $\"nbre_velos_meca\", $\"dist_km\", $\"esperance\")\n",
    "    .distinct().sort($\"esperance\".desc).show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url: String = https://opendata.paris.fr/api/records/1.0/search/?dataset=velib-disponibilite-en-temps-reel&facet=records&rows=1391\n",
       "data: String = {\"nhits\": 1391, \"parameters\": {\"dataset\": \"velib-disponibilite-en-temps-reel\", \"timezone\": \"UTC\", \"rows\": 1391, \"format\": \"json\", \"facet\": [\"records\"]}, \"records\": [{\"datasetid\": \"velib-disponibilite-en-temps-reel\", \"recordid\": \"20e1277cebc85e0683be0b225f80798c4fb93f74\", \"fields\": {\"nbfreeedock\": 31, \"station_state\": \"Operative\", \"maxbikeoverflow\": 0, \"creditcard\": \"no\", \"station_type\": \"yes\", \"overflowactivation\": \"no\", \"station_code\": \"16107\", \"overflow\": \"no\", \"nbbikeoverflow\": 0, \"duedate\": \"2018-07-15\", \"densitylevel\": \"1\", \"nbedock\": 35, \"station\": \"{\\\"code\\\": \\\"16107\\\", \\\"name\\\": \\\"Benjamin Godard - Victor Hugo\\\", \\\"state\\\": \\\"Operative\\..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val url = \"https://opendata.paris.fr/api/records/1.0/search/?dataset=velib-disponibilite-en-temps-reel&facet=overflowactivation&facet=creditcard&facet=kioskstate&facet=station_state\" \n",
    "val url = \"https://opendata.paris.fr/api/records/1.0/search/?dataset=velib-disponibilite-en-temps-reel&facet=records&rows=1391\"\n",
    "val data = scala.io.Source.fromURL(url).mkString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsed: Option[Any] = Some(Map(nhits -> 1391.0, parameters -> Map(format -> json, timezone -> UTC, dataset -> velib-disponibilite-en-temps-reel, facet -> List(records), rows -> 1391.0), records -> List(Map(record_timestamp -> 2019-11-04T22:07:00.570000+00:00, fields -> Map(nbfreedock -> 0.0, station_type -> yes, overflow -> no, station_code -> 16107, station_name -> Benjamin Godard - Victor Hugo, nbfreeedock -> 31.0, duedate -> 2018-07-15, geo -> List(48.865983, 2.275725), nbdock -> 0.0, maxbikeoverflow -> 0.0, overflowactivation -> no, densitylevel -> 1, nbebike -> 2.0, kioskstate -> yes, station_state -> Operative, nbebikeoverflow -> 0.0, nbbikeoverflow -> 0.0, nbedock -> 35.0, creditcard -> no, station -> {\"code\": \"16107\", \"name\": \"Benjamin Godard - Victor Hugo\", \"state\": \"Operative\"..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsed = JSON.parseFull(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:40: warning: non-variable type argument Any in type pattern scala.collection.immutable.Map[Any,Any] (the underlying of Map[Any,Any]) is unchecked since it is eliminated by erasure\n",
       "         case Some(e:Map[Any,Any]) => e(\"records\")\n",
       "                     ^\n",
       "<console>:39: warning: match may not be exhaustive.\n",
       "It would fail on the following input: Some((x: Any forSome x not in scala.collection.immutable.Map[?,?]))\n",
       "       val listAsAny = parsed match {\n",
       "                       ^\n",
       "listAsAny: Any = List(Map(record_timestamp -> 2019-11-04T22:07:00.570000+00:00, fields -> Map(nbfreedock -> 0.0, station_type -> yes, overflow -> no, station_code -> 16107, station_name -> Benjamin Godard - Victor Hugo, nbfreeedock -> 31.0, duedate -> 2018-07-15, geo -> List(48.865983, 2.275725), nbdock -> 0.0, maxbikeoverflow -> 0.0, overflowactivation -> no, densitylevel -> 1, nbebike -> 2.0, kioskstate -> yes, station_state -> Operative, nbebikeoverflow -> 0.0, nbbikeoverflow -> 0.0, nbedock -> 35.0, creditcard -> no, station -> {\"code\": \"16107\", \"name\": \"Benjamin Godard - Victor Hugo\", \"state\": \"Operative\", \"type\": \"yes\", \"dueDate\": 1531632033, \"gps\": {\"latitude\": 48.865983, \"longitude\": 2.275725}}, nbbike -> 2.0), recordid -> 20e1277cebc85e0683be0b225f80798c4fb93f74, geometry -> Ma..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val listAsAny = parsed match {\n",
    "  case Some(e:Map[Any,Any]) => e(\"records\")\n",
    "  case None => println(\"Failed.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recordsList: List[Map[String,Any]] = List(Map(record_timestamp -> 2019-11-04T22:07:00.570000+00:00, fields -> Map(nbfreedock -> 0.0, station_type -> yes, overflow -> no, station_code -> 16107, station_name -> Benjamin Godard - Victor Hugo, nbfreeedock -> 31.0, duedate -> 2018-07-15, geo -> List(48.865983, 2.275725), nbdock -> 0.0, maxbikeoverflow -> 0.0, overflowactivation -> no, densitylevel -> 1, nbebike -> 2.0, kioskstate -> yes, station_state -> Operative, nbebikeoverflow -> 0.0, nbbikeoverflow -> 0.0, nbedock -> 35.0, creditcard -> no, station -> {\"code\": \"16107\", \"name\": \"Benjamin Godard - Victor Hugo\", \"state\": \"Operative\", \"type\": \"yes\", \"dueDate\": 1531632033, \"gps\": {\"latitude\": 48.865983, \"longitude\": 2.275725}}, nbbike -> 2.0), recordid -> 20e1277cebc85e0683be0b225f80798c4fb9..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recordsList = listAsAny.asInstanceOf[List[Map[String, Any]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recordsFields: List[Any] = List(Map(nbfreedock -> 0.0, station_type -> yes, overflow -> no, station_code -> 16107, station_name -> Benjamin Godard - Victor Hugo, nbfreeedock -> 31.0, duedate -> 2018-07-15, geo -> List(48.865983, 2.275725), nbdock -> 0.0, maxbikeoverflow -> 0.0, overflowactivation -> no, densitylevel -> 1, nbebike -> 2.0, kioskstate -> yes, station_state -> Operative, nbebikeoverflow -> 0.0, nbbikeoverflow -> 0.0, nbedock -> 35.0, creditcard -> no, station -> {\"code\": \"16107\", \"name\": \"Benjamin Godard - Victor Hugo\", \"state\": \"Operative\", \"type\": \"yes\", \"dueDate\": 1531632033, \"gps\": {\"latitude\": 48.865983, \"longitude\": 2.275725}}, nbbike -> 2.0), Map(nbfreedock -> 2.0, station_type -> yes, overflow -> no, station_code -> 6015, station_name -> André Mazet - Saint-André de..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recordsFields = recordsList.map(_(\"fields\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "champs: List[Map[String,String]] = List(Map(nbfreedock -> 0.0, station_type -> yes, overflow -> no, station_code -> 16107, station_name -> Benjamin Godard - Victor Hugo, nbfreeedock -> 31.0, duedate -> 2018-07-15, geo -> List(48.865983, 2.275725), nbdock -> 0.0, maxbikeoverflow -> 0.0, overflowactivation -> no, densitylevel -> 1, nbebike -> 2.0, kioskstate -> yes, station_state -> Operative, nbebikeoverflow -> 0.0, nbbikeoverflow -> 0.0, nbedock -> 35.0, creditcard -> no, station -> {\"code\": \"16107\", \"name\": \"Benjamin Godard - Victor Hugo\", \"state\": \"Operative\", \"type\": \"yes\", \"dueDate\": 1531632033, \"gps\": {\"latitude\": 48.865983, \"longitude\": 2.275725}}, nbbike -> 2.0), Map(nbfreedock -> 2.0, station_type -> yes, overflow -> no, station_code -> 6015, station_name -> André Mazet - Saint-..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val champs = recordsFields.asInstanceOf[List[Map[String, String]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfrdd: List[(String, String, String, String, String, String, String, String)] = List((Benjamin Godard - Victor Hugo,16107,31.0,35.0,0.0,2.0,0.0,2.0), (André Mazet - Saint-André des Arts,6015,40.0,52.0,2.0,4.0,3.0,8.0), (Faubourg Du Temple - Republique,11037,26.0,38.0,0.0,2.0,0.0,9.0), (Toudouze - Clauzel,9020,9.0,21.0,0.0,6.0,0.0,6.0), (Charonne - Robert et Sonia Delauney,11104,17.0,20.0,0.0,2.0,0.0,1.0), (Mairie du 12ème,12109,5.0,30.0,0.0,16.0,0.0,9.0), (Square Boucicaut,7003,41.0,60.0,0.0,15.0,0.0,3.0), (Harpe - Saint-Germain,5001,1.0,1.0,38.0,0.0,44.0,6.0), (Jourdan - Stade Charléty,14014,45.0,60.0,0.0,7.0,0.0,7.0), (Jouffroy d'Abbans - Wagram,17026,0.0,0.0,29.0,7.0,41.0,5.0), (Guersant - Gouvion-Saint-Cyr,17041,13.0,38.0,1.0,9.0,1.0,16.0), (Alibert - Jemmapes,10013,40.0,59.0,1.0,11..."
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfrdd = champs.map{x => (x(\"station_name\"), x(\"station_code\"), x(\"nbfreeedock\"), x(\"nbedock\"), x(\"nbfreedock\"), x(\"nbbike\"), x(\"nbdock\"), x(\"nbebike\"))}\n",
    "val rdd = sc.makeRDD(dfrdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(nom_station,StringType,true), StructField(cod_station,DoubleType,true), StructField(nbre_bornes_dispo_elec,DoubleType,true), StructField(nbre_bornes_station_elec,DoubleType,true), StructField(nbre_bornes_dispo,DoubleType,true), StructField(nbre_velos_meca,DoubleType,true), StructField(nbre_bornes_station,DoubleType,true), StructField(nbre_velos_elec,DoubleType,true))\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = new StructType()\n",
    "  .add(StructField(\"nom_station\", StringType, true))\n",
    "  .add(StructField(\"cod_station\", DoubleType, true))\n",
    "  .add(StructField(\"nbre_bornes_dispo_elec\", DoubleType, true))\n",
    "  .add(StructField(\"nbre_bornes_station_elec\", DoubleType, true))\n",
    "  .add(StructField(\"nbre_bornes_dispo\", DoubleType, true))\n",
    "  .add(StructField(\"nbre_velos_meca\", DoubleType, true))\n",
    "  .add(StructField(\"nbre_bornes_station\", DoubleType, true))\n",
    "  .add(StructField(\"nbre_velos_elec\", DoubleType, true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velibTempReel: org.apache.spark.sql.DataFrame = [nom_station: string, cod_station: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velibTempReel = rdd.toDF(\"nom_station\", \"cod_station\", \"nbre_bornes_dispo_elec\", \"nbre_bornes_station_elec\", \"nbre_bornes_dispo\", \"nbre_velos_meca\", \"nbre_bornes_station\", \"nbre_velos_elec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nom_station: string (nullable = true)\n",
      " |-- cod_station: string (nullable = true)\n",
      " |-- nbre_bornes_dispo_elec: string (nullable = true)\n",
      " |-- nbre_bornes_station_elec: string (nullable = true)\n",
      " |-- nbre_bornes_dispo: string (nullable = true)\n",
      " |-- nbre_velos_meca: string (nullable = true)\n",
      " |-- nbre_bornes_station: string (nullable = true)\n",
      " |-- nbre_velos_elec: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velibTempReel.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 1391\n",
      "Nombre de colonnes : 8\n"
     ]
    }
   ],
   "source": [
    "println(s\"Nombre de lignes : ${velibTempReel.count}\")\n",
    "println(s\"Nombre de colonnes : ${velibTempReel.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 1252, localhost, executor driver): java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.String",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 1252, localhost, executor driver): java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.String",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.serializefromobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)",
      "  ... 42 elided",
      "Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.String",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.serializefromobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "velibTempReel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: Array[(String, String, String, String, String, String, String, String)] = Array((Benjamin Godard - Victor Hugo,16107,31.0,35.0,0.0,2.0,0.0,2.0), (André Mazet - Saint-André des Arts,6015,40.0,52.0,2.0,4.0,3.0,8.0), (Faubourg Du Temple - Republique,11037,26.0,38.0,0.0,2.0,0.0,9.0), (Toudouze - Clauzel,9020,9.0,21.0,0.0,6.0,0.0,6.0), (Charonne - Robert et Sonia Delauney,11104,17.0,20.0,0.0,2.0,0.0,1.0))\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "42: warning: fruitless type test: a value of type (String, String, String, String, String, String, String, String) cannot also be a Array[T]",
     "output_type": "error",
     "traceback": [
      "<console>:42: warning: fruitless type test: a value of type (String, String, String, String, String, String, String, String) cannot also be a Array[T]",
      "         case Array(s0, s1, s2, s3, s4, s5, s6, s7) => schema(s0, s1, s2, s3, s4, s5, s6, s7) }.toDF()",
      "                   ^",
      "<console>:42: error: pattern type is incompatible with expected type;",
      " found   : Array[T]",
      " required: (String, String, String, String, String, String, String, String)",
      "         case Array(s0, s1, s2, s3, s4, s5, s6, s7) => schema(s0, s1, s2, s3, s4, s5, s6, s7) }.toDF()",
      "                   ^",
      "<console>:42: error: overloaded method value apply with alternatives:",
      "  (fieldIndex: Int)org.apache.spark.sql.types.StructField <and>",
      "  (names: Set[String])org.apache.spark.sql.types.StructType <and>",
      "  (name: String)org.apache.spark.sql.types.StructField",
      " cannot be applied to (<error>, <error>, <error>, <error>, <error>, <error>, <error>, <error>)",
      "         case Array(s0, s1, s2, s3, s4, s5, s6, s7) => schema(s0, s1, s2, s3, s4, s5, s6, s7) }.toDF()",
      "                                                       ^",
      ""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
